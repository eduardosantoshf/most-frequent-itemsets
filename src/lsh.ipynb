{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/22 22:50:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkFiles\n",
    "import re\n",
    "import random\n",
    "\n",
    "\n",
    "path = \"../data/covid_news_truncated_2.json\"\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.getAll()\n",
    "\n",
    "sc = SparkContext(appName=\"lsh\")\n",
    "    \n",
    "spark = SparkSession(sc)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "textfile = sc.textFile(path)\n",
    "\n",
    "#sc.addFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1346071551842594817', '1346067714662670336', 0.28205128205128205)\n",
      "('1346071551842594817', '1346023661002752000', 0.25)\n",
      "('1346071551842594817', '1346023652303757312', 0.3157894736842105)\n",
      "('1346071551842594817', '1346023643663495168', 0.35135135135135137)\n",
      "('1346071551842594817', '1346026165308370944', 0.3422818791946309)\n",
      "('1346071551842594817', '1346027458827608065', 0.2903225806451613)\n",
      "('1346071551842594817', '1345941019414720512', 0.27388535031847133)\n",
      "('1346071551842594817', '1346074073105887233', 0.26582278481012656)\n",
      "('1346071551842594817', '1346074071608532994', 0.30718954248366015)\n",
      "('1346071551842594817', '1346073787918331905', 0.35135135135135137)\n"
     ]
    }
   ],
   "source": [
    "# Large prime value to use in hashing\n",
    "LARGE_PRIME = 75874811\n",
    "\n",
    "# number of hash functions\n",
    "K_VALUE = 100\n",
    "R_VALUE = 5\n",
    "B_VALUE = 20\n",
    "\n",
    "# generate random array to use in hashing\n",
    "RANDOM_ARRAY = [(random.randint(1,2**31 - 1), random.randint(1,2**31 - 1)) for i in range(0,K_VALUE)]\n",
    "\n",
    "\n",
    "# gets document and returns k-shingles\n",
    "def shingles(document, k=9):\n",
    "\n",
    "    #initial set\n",
    "    shingles_set = set()\n",
    "\n",
    "    # getting rid of punctuation, etc\n",
    "    document[1] = re.sub(r'[^\\w\\s]', '', document[1].lower())\n",
    "\n",
    "    # split to chars\n",
    "    chars_list = re.split('', document[1].lower())\n",
    "\n",
    "    # create shingles with length k\n",
    "    for i in range(len(chars_list) - k):\n",
    "        chars = chars_list[i:i + k]\n",
    "        shingle = ''.join(chars)\n",
    "        shingles_set.add(shingle)\n",
    "\n",
    "    # sort shingles\n",
    "    shingles_set = sorted(shingles_set)\n",
    "\n",
    "    # returns: doc_id, set_shingles\n",
    "    return document[0], shingles_set\n",
    "\n",
    "\n",
    "# hash a value\n",
    "def hash_function(x, a, b, len_x): \n",
    "    return (((a * hash(x) + b) % LARGE_PRIME ) % len_x)\n",
    "\n",
    "# get signature matrix from min hashing\n",
    "def min_hash(document):\n",
    "    \n",
    "    # shingles\n",
    "    x = document[1]\n",
    "\n",
    "    # initial matrix\n",
    "    signature_matrix = []\n",
    "\n",
    "    # iterate through the defined number of hash functions (hi)\n",
    "    for i in range(0, K_VALUE):\n",
    "\n",
    "        # start value is infinite\n",
    "        minhash = float('inf')\n",
    "\n",
    "        # get random integers\n",
    "        a,b = RANDOM_ARRAY[i]\n",
    "\n",
    "        # for each shingle\n",
    "        for value in x:\n",
    "            # hash shingle\n",
    "            h = hash_function(value,a,b, len(x))\n",
    "            # if lower, replace with current value\n",
    "            if h < minhash:\n",
    "                minhash = h\n",
    "\n",
    "        # append the lowest number\n",
    "        signature_matrix.append(minhash)\n",
    "\n",
    "    #print(signature_matrix)\n",
    "    \n",
    "    # returns: doc_id, signature matrix\n",
    "    return document[0],signature_matrix\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "# gets a band and hashes it to a bucket\n",
    "def hash_lsh(band): \n",
    "\n",
    "    # intial array\n",
    "    h1_array = []\n",
    "\n",
    "    # for each row within the band\n",
    "    for value in band:\n",
    "        #print(value)\n",
    "        #print((value * len(band)) % LARGE_PRIME)\n",
    "        #print((421*value + 16) % 1013)\n",
    "        #print(\"------\")\n",
    "        # hash\n",
    "        h1_array.append((value * len(band)) % LARGE_PRIME)\n",
    "\n",
    "    #returns: min value\n",
    "    return min(h1_array)\n",
    "\n",
    "# gets signature and returns bucket values for each band\n",
    "def get_bucket_values(signature):\n",
    "\n",
    "\n",
    "    # list of bucket values\n",
    "    bucket_values = []\n",
    "\n",
    "    # for the entire signature, iterate over each band with r rows and hash it\n",
    "    for idx in range(0, B_VALUE):\n",
    "\n",
    "        # if there is no more bands to hash\n",
    "        if idx * R_VALUE > len(signature): \n",
    "            break \n",
    "\n",
    "        # get end of band\n",
    "        max_id = min(idx * R_VALUE + R_VALUE, len(signature))\n",
    "\n",
    "        # hash the band to a bucket\n",
    "        bucket = hash_lsh(signature[idx * R_VALUE : max_id])\n",
    "\n",
    "        # append bucket value\n",
    "        bucket_values.append(bucket)\n",
    "\n",
    "    # returns: doc_id, bucket values\n",
    "    return bucket_values\n",
    "\n",
    "# given the signatures, returns candidate pairs\n",
    "def lsh_algorithm(signatures_matrix):\n",
    "\n",
    "    # dict with buckets for each document\n",
    "    k_buckets = {}\n",
    "\n",
    "    # initial candidates list\n",
    "    candidates = []\n",
    "\n",
    "    # iterate over the signatures\n",
    "    for doc in signatures_matrix:\n",
    "\n",
    "        # get the bucket values for each signature\n",
    "        bucket = get_bucket_values(signatures_matrix[doc])\n",
    "        #print(bucket)\n",
    "\n",
    "        # iterate over the other signatures bucket values\n",
    "        for b_doc in k_buckets:\n",
    "\n",
    "            # iterate over the bucket values\n",
    "            for i in range(len(bucket)):\n",
    "\n",
    "                # if at least 1 bucket value is the same, then at least 1 band hashes to the same bucket -> candidate \n",
    "                if k_buckets[b_doc][i] == bucket[i]:\n",
    "\n",
    "                    # because it is candidate, compare with jaccard similarity and append to candidates list\n",
    "                    similar_pair = jaccard_similarity(signatures_matrix[doc], signatures_matrix[b_doc])\n",
    "                    candidates.append((doc, b_doc, similar_pair))\n",
    "\n",
    "                    # because we only need 1 hash value in the same bucket, no need to continue\n",
    "                    break\n",
    "\n",
    "        # add the bucket values for each signature\n",
    "        k_buckets[doc] = bucket\n",
    "\n",
    "    # returns: candidates\n",
    "    return candidates\n",
    "\n",
    "# calculate jaccard similarity\n",
    "def jaccard_similarity(sig_matrix_1, sig_matrix_2):\n",
    "    # get intersection of the 2 matrices\n",
    "    intersection = len([sig_matrix_1[i] for i in range(0, len(sig_matrix_1)) if (sig_matrix_1[i] == sig_matrix_2[i])])\n",
    "    #intersection_2 = len([sig_matrix_1[i] for i in range(0, len(sig_matrix_1)) if (sig_matrix_1[i] == sig_matrix_2[i]) and (sig_matrix_1[i] != 0)])\n",
    "    # get union of the 2 matrices\n",
    "    #union_2 = len([sig_matrix_1[i] for i in range(0, len(sig_matrix_1)) if (sig_matrix_1[i] != 0) or (sig_matrix_2[i] != 0)])\n",
    "    union = (len(sig_matrix_1) + len(sig_matrix_2)) - intersection\n",
    "\n",
    "    # calculate jaccard similarity\n",
    "    jaccard_sim = intersection / union\n",
    "    #jaccard_sim_2 = intersection_2 / union_2\n",
    "\n",
    "    return jaccard_sim\n",
    "\n",
    "\n",
    "final = textfile.map(lambda line: eval(line)) \\\n",
    "                .map(lambda dict: [dict[\"tweet_id\"], dict[\"text\"]]) \\\n",
    "                .map(shingles) \\\n",
    "                .map(min_hash)\n",
    "\n",
    "#print(\"shingles and minhash done\")\n",
    "\n",
    "signatures_matrix = { doc: sig_matrix for doc, sig_matrix in final.collect() }\n",
    "#\n",
    "#print(\"collect done\")\n",
    "#\n",
    "#print(signatures_matrix)\n",
    "\n",
    "#y = sc.parallelize(final)\n",
    "#x = final.collect()\n",
    "#print(x)\n",
    "\n",
    "candidates = lsh_algorithm(signatures_matrix)\n",
    "#\n",
    "#print(\"similar pairs found\")\n",
    "#\n",
    "sorted_candidates = sc.parallelize(candidates).sortBy(lambda pair: - pair[2])\n",
    "#\n",
    "#print(\"final pairs done\")\n",
    "#\n",
    "final_results = sorted_candidates.collect()\n",
    "#\n",
    "for x in final_results[-10:]:\n",
    "    print(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

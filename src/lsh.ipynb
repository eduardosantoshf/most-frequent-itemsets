{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of spark and path definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/23 23:06:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import re\n",
    "import random\n",
    "\n",
    "path = \"../data/covid_news_truncated.json\"\n",
    "\n",
    "sc = SparkContext(appName=\"lsh\")\n",
    "    \n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "textfile = sc.textFile(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1\n",
    "\n",
    "Main program for the LSH algorithm.\n",
    "\n",
    "r and b value were found so that > 90% pairs with 85% similarity and < 5% of pairs with 60% similarity were found.\n",
    "\n",
    "r = 13\n",
    "b = 11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Large prime value to use in hashing\n",
    "PRIME_NUMBER = 50000017\n",
    "\n",
    "# r and b values for > 90% with 85% similarity and < 5% for 60% similarity\n",
    "R_VALUE = 13\n",
    "B_VALUE = 11\n",
    "\n",
    "# number of hash functions\n",
    "K_VALUE = R_VALUE * B_VALUE\n",
    "\n",
    "# generate random array to use in hashing\n",
    "RANDOMS = [(random.randint(1,2**31 - 1), random.randint(1,2**31 - 1)) for i in range(0,K_VALUE)]\n",
    "\n",
    "# gets document and returns k-shingles\n",
    "def shingles(document, k=9):\n",
    "\n",
    "    #initial set\n",
    "    shingles_set = set()\n",
    "\n",
    "    # getting rid of punctuation, etc\n",
    "    document[1] = re.sub(r'[^\\w\\s]', '', document[1])\n",
    "\n",
    "    # split to chars\n",
    "    chars_list = re.split('', document[1].lower())\n",
    "\n",
    "    # create shingles with length k\n",
    "    for i in range(len(chars_list) - k):\n",
    "        chars = chars_list[i : i + k]\n",
    "        shingle = ''.join(chars)\n",
    "        shingles_set.add(shingle)\n",
    "\n",
    "    # returns: doc_id, set_shingles\n",
    "    return document[0], shingles_set\n",
    "\n",
    "# hash a value\n",
    "def hash_function(x, a, b, N): \n",
    "    return (((a * hash(x) + b) % PRIME_NUMBER ) % N)\n",
    "\n",
    "# get signature matrix from min hashing\n",
    "def min_hash(document, total_size=910000):\n",
    "    \n",
    "    # shingles\n",
    "    x = document[1]\n",
    "\n",
    "    # initial matrix\n",
    "    signature_matrix = []\n",
    "\n",
    "    # iterate through the defined number of hash functions (hi)\n",
    "    for i in range(0, K_VALUE):\n",
    "\n",
    "        # start value is infinite\n",
    "        minhash = float('inf')\n",
    "\n",
    "        # get random integers\n",
    "        a,b = RANDOMS[i]\n",
    "\n",
    "        N = total_size\n",
    "        # for each shingle\n",
    "        for value in x:\n",
    "            # hash shingle\n",
    "            h = hash_function(value, a, b, N)\n",
    "            # if lower, replace with current value\n",
    "            if h < minhash:\n",
    "                minhash = h\n",
    "\n",
    "        # append the lowest number\n",
    "        signature_matrix.append(minhash)\n",
    "    \n",
    "    # returns: doc_id, signature matrix\n",
    "    return document[0], signature_matrix\n",
    "\n",
    "# gets a band and hashes it to a bucket\n",
    "def hash_lsh(band): \n",
    "\n",
    "    # intial array\n",
    "    hashes = []\n",
    "\n",
    "    # for each row within the band\n",
    "    for r in band:\n",
    "        # hash\n",
    "        hashes.append((r * len(band)) % PRIME_NUMBER)\n",
    "\n",
    "    #returns: min value\n",
    "    return min(hashes)\n",
    "\n",
    "# gets signature and returns bucket values for each band\n",
    "def get_bucket_values(signature):\n",
    "\n",
    "    # list of bucket values\n",
    "    bucket_values = []\n",
    "\n",
    "    # for the entire signature, iterate over each band with r rows and hash it\n",
    "    for idx in range(0, B_VALUE):\n",
    "        \n",
    "        \n",
    "        start_of_band = idx * R_VALUE\n",
    "\n",
    "        # if there is no more bands to hash\n",
    "        if start_of_band > len(signature): \n",
    "            break \n",
    "\n",
    "        # get end of band\n",
    "        end_of_band = min(start_of_band + R_VALUE, len(signature))\n",
    "\n",
    "        # hash the band to a bucket\n",
    "        bucket = hash_lsh(signature[start_of_band : end_of_band])\n",
    "\n",
    "        # append bucket value\n",
    "        bucket_values.append(bucket)\n",
    "\n",
    "    # returns: doc_id, bucket values\n",
    "    return bucket_values\n",
    "\n",
    "# given the signatures, returns candidate pairs\n",
    "def lsh_algorithm(signatures_matrix):\n",
    "\n",
    "    # dict with buckets for each document\n",
    "    k_buckets = {}\n",
    "\n",
    "    # initial candidates list\n",
    "    candidates = []\n",
    "\n",
    "    # iterate over the signatures\n",
    "    for doc in signatures_matrix:\n",
    "\n",
    "        # get the bucket values for each signature\n",
    "        bucket = get_bucket_values(signatures_matrix[doc])\n",
    "\n",
    "        # iterate over the other signatures bucket values\n",
    "        for b_doc in k_buckets:\n",
    "\n",
    "            # iterate over the bucket values\n",
    "            for i in range(len(bucket)):\n",
    "\n",
    "                # if at least 1 bucket value is the same, then at least 1 band hashes to the same bucket -> candidate \n",
    "                if k_buckets[b_doc][i] == bucket[i]:\n",
    "\n",
    "                    # because it is candidate, compare with jaccard similarity and append to candidates list\n",
    "                    jacc_sim = jaccard_similarity(signatures_matrix[doc], signatures_matrix[b_doc])\n",
    "                    candidates.append((doc, b_doc, jacc_sim))\n",
    "\n",
    "                    # because we only need 1 hash value in the same bucket, no need to continue\n",
    "                    break\n",
    "\n",
    "        # add the bucket values for each signature\n",
    "        k_buckets[doc] = bucket\n",
    "\n",
    "    # returns: candidates\n",
    "    return candidates\n",
    "\n",
    "# calculate jaccard similarity\n",
    "def jaccard_similarity(sig_matrix_1, sig_matrix_2):\n",
    "    # get intersection of the 2 matrices\n",
    "    intersection = len([sig_matrix_1[i] for i in range(0, len(sig_matrix_1)) if (sig_matrix_1[i] == sig_matrix_2[i])])\n",
    "    # get union of the 2 matrices\n",
    "    union = (len(sig_matrix_1) + len(sig_matrix_2)) - intersection\n",
    "    # calculate jaccard similarity\n",
    "    jaccard_sim = intersection / union\n",
    "\n",
    "    return jaccard_sim\n",
    "\n",
    "\n",
    "\n",
    "final_shingles = textfile.map(lambda line: eval(line)) \\\n",
    "                .map(lambda dict: [dict[\"tweet_id\"], dict[\"text\"]]) \\\n",
    "                .map(shingles)\n",
    "\n",
    "\n",
    "final_minhash = final_shingles.map(min_hash)\n",
    "\n",
    "\n",
    "signatures_matrix = { document: sig_matrix for document, sig_matrix in final_minhash.collect() }\n",
    "\n",
    "candidates = lsh_algorithm(signatures_matrix)\n",
    "sorted_candidates = sc.parallelize(candidates).sortBy(lambda pair: - pair[2])\n",
    "\n",
    "final_results = sorted_candidates.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2\n",
    "\n",
    "For a given article, returns the articles at least 85% similar. Uses pre-processed results obtained above in LSH, and tests them with Jaccard similarity between shingles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1349823098623819784 jaccard sim with 1349669108229533696: 0.9942140790742526\n",
      "1349823098623819784 jaccard sim with 1349654208337862656: 0.9942140790742526\n",
      "1349823098623819784 jaccard sim with 1349454372560920578: 0.9942140790742526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# jaccar similarity for shingles\n",
    "def shingles_jaccard(shingles_1, shingles_2):\n",
    "    # intersection\n",
    "    intersection = len(list(set(shingles_1).intersection(shingles_2)))\n",
    "    #union\n",
    "    union = (len(set(shingles_1)) + len(set(shingles_2))) - intersection\n",
    "    \n",
    "    # jaccard\n",
    "    return float(intersection) / union\n",
    "\n",
    "\n",
    "# similarity of article with possible candidates\n",
    "def article_shingles_similarity(doc_id, filtered_shingles):\n",
    "    for x in filtered_shingles:\n",
    "        if x[0] == str(doc_id):\n",
    "            doc_shingles = x[1]\n",
    "\n",
    "    for candidate in filtered_shingles:\n",
    "        if str(doc_id) != candidate[0]:\n",
    "            jacc = shingles_jaccard(doc_shingles, candidate[1])\n",
    "            if jacc > 0.85:\n",
    "                print(str(doc_id) + \" jaccard sim with \" + candidate[0] + \": \" + str(jacc))\n",
    "\n",
    "\n",
    "\n",
    "article = 1349823098623819784\n",
    "filtered_candidates = sorted_candidates.filter(lambda x: x[0] == str(article)).collect()\n",
    "\n",
    "if filtered_candidates:\n",
    "    candidates_ids = [filtered_candidates[0][0]] + [x[1] for x in filtered_candidates]\n",
    "    filtered_shingles = final_shingles.filter(lambda x: x[0] in candidates_ids).collect()\n",
    "\n",
    "    article_shingles_similarity(article, filtered_shingles)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3\n",
    "\n",
    "Computes the false positives and false negatives from the LSH algorithm by calculating the jaccard similarities and checking if the pair is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positives: 0.0 %\n",
      "False negatives: 2.1791460788040036 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def false_positives(signatures_matrix, candidates):\n",
    "\n",
    "    count = 0\n",
    "    for pair in candidates:\n",
    "\n",
    "        matrix_1 = signatures_matrix[pair[0]]\n",
    "        matrix_2 = signatures_matrix[pair[1]]\n",
    "\n",
    "        matrix_similarity = jaccard_similarity(matrix_1, matrix_2)\n",
    "\n",
    "        if pair[2] > 0.85 and matrix_similarity < 0.85: \n",
    "            count += 1\n",
    "\n",
    "    percentage = count/len(candidates)\n",
    "\n",
    "    return percentage\n",
    "\n",
    "def false_negatives(signatures_matrix, candidates):\n",
    "\n",
    "    good_pairs = []\n",
    "    docs = list(signatures_matrix)\n",
    "    \n",
    "    for i in range(len(docs)):\n",
    "        for k in range(i + 1, len(docs)):\n",
    "            \n",
    "            matrix_1 = signatures_matrix[docs[i]]\n",
    "            matrix_2 = signatures_matrix[docs[k]]\n",
    "            matrix_similarity = jaccard_similarity(matrix_1, matrix_2)\n",
    "            \n",
    "            if matrix_similarity > 0.85:\n",
    "                good_pairs.append((docs[i],docs[k]))\n",
    "\n",
    "    tuple_pairs = [(x[0], x[1]) for x in candidates]\n",
    "\n",
    "    false_negatives = len(set(good_pairs) - set(tuple_pairs))\n",
    "    return false_negatives/len(candidates)\n",
    "\n",
    "false_positives = false_positives(signatures_matrix, candidates)\n",
    "false_negatives = false_negatives(signatures_matrix, candidates)\n",
    "\n",
    "print(\"False positives: \" + str(false_positives * 100) + \" %\")\n",
    "print(\"False negatives: \" + str(false_negatives * 100) + \" %\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
